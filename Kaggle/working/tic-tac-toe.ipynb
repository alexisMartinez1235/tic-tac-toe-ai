{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras-rl2\n# !pip install --upgrade ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T01:24:14.232006Z","iopub.execute_input":"2022-06-29T01:24:14.232550Z","iopub.status.idle":"2022-06-29T01:24:25.911285Z","shell.execute_reply.started":"2022-06-29T01:24:14.232510Z","shell.execute_reply":"2022-06-29T01:24:25.909802Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom gym.envs.registration import register\n\n# math packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport matplotlib.pyplot as plt\nimport math\n\n# openai packages\nfrom gym import Env\nfrom gym.spaces import Discrete, Tuple\n\n# Neural network packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import models, layers\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# system packagse\nimport os\nimport sys\nimport time\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b8834607-8bc7-4fd6-8513-0ce2deacf8c5","_cell_guid":"a94b54b5-f177-41fd-9d5f-f9dc017806d5","execution":{"iopub.status.busy":"2022-06-29T02:24:04.977111Z","iopub.execute_input":"2022-06-29T02:24:04.977641Z","iopub.status.idle":"2022-06-29T02:24:04.994601Z","shell.execute_reply.started":"2022-06-29T02:24:04.977601Z","shell.execute_reply":"2022-06-29T02:24:04.993580Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# 1. I create the Tic tac toe class and the base configuration of the player","metadata":{"_uuid":"20502030-8174-49ec-b141-4fdc110bb64b","_cell_guid":"37d42075-8b91-445a-94ec-d2ff17fdd76b","trusted":true}},{"cell_type":"code","source":"class TicTacToe(Env):\n    metadata = {\n        \"render_modes\": [\n            \"human\"\n        ],\n        \"render_fps\": 4\n    }\n\n    def __init__(self, players: int, table: np.matrix, lengthOfLine: int, allowDebug: bool):\n        self.fields = table.shape[0]\n        self.cols = table.shape[1]\n        \n        # define personalized environment gym props\n        self.table = table # has a state role \n        \n        #self.action_space = Tuple((Discrete(self.fields), Discrete(self.cols))) # for delete tuples used after\n        self.action_space = Discrete(self.fields * self.cols) # for delete tuples used after\n        \n        # self.action_space = Box(low=-1.0, high=2.0, shape=(self.fields, self.cols), dtype=np.float32)\n        #self.action_space.sample()\n        self.numOfPlayers = players\n        \n        self.reset(fillTable=False)\n\n        \n        self.secondTransform = np.flip(np.identity(3), axis=1)\n        self.redirectPrintTo = sys.stdout\n        self.lengthOfLine = lengthOfLine\n        \n        if not allowDebug:\n            self.redirectPrintTo = open(os.devnull, 'w')\n    \n    def _get_obs(self):\n        return self.table.reshape((1, 9)).tolist()[0]\n        # return tf.convert_to_tensor(\n        #  [self.table.reshape((1,9)).tolist()],\n        #  dtype=tf.int32\n        #)\n    \n    def reset(self, fillTable=True):\n        if fillTable:\n            self.table.fill(0)\n        self.time = 0\n        self.winner = 0\n        self.turn = random.choices([1, self.numOfPlayers])[0]\n        self.history = tf.Variable(tf.zeros(shape=(self.fields * self.cols + 1, self.fields, self.cols), dtype=tf.dtypes.int32))\n        self.saveToHistory()\n      \n        #return tf.convert_to_tensor(\n        #  [self.table.reshape((1,9)).tolist()],\n        #  dtype=tf.int32\n        #), {\"status\": \"None\"}\n        return self.table.reshape((1, 9)).tolist()[0]\n\n    def __deepcopy__(self, memodict={}):\n        cpyobj = type(self)() # shallow copy of whole object \n        cpyobj.deep_cp_attr = copy.deepcopy(self.other_attr, memodict) # deepcopy required attr\n\n        return cpyobj\n    \n    def saveToHistory(self):\n        self.history[self.time].assign(self.table)\n        self.time += 1 \n            \n    def changeTurn(self):\n        if self.turn < self.numOfPlayers:\n            self.turn = self.turn + 1\n        else:\n            self.turn = 1\n\n    def hasOwner(self, field: int, col: int):\n        return self.table.item(field, col) != 0\n \n\n    def crossOutCell(self, field: int, col: int, idPlayer: int) -> bool:        \n        if field < self.table.shape[0] and \\\n            col < self.table.shape[1] and 0 <= field and 0 <= col:\n            if not self.hasOwner(field, col):\n                if self.turn == idPlayer:\n                    self.table.itemset((field, col), self.turn)\n                    self.saveToHistory()\n                    self.changeTurn()\n        else:\n              print(\"invalid input\", file=self.redirectPrintTo)\n        return False\n\n    def headerTable(self, players):\n        print(str(self.turn) + \", wait for checking to play\", file=self.redirectPrintTo)\n        for player in players:\n            print(str(player.idPlayer)+\":\" + player.name, file=self.redirectPrintTo)\n        print(\"\", file=self.redirectPrintTo)\n\n    def formatTable(self, players, showHeader: bool = False):\n        if showHeader:\n              self.headerTable(players)\n        for f in range(0, self.table.shape[0]):\n            for c in range(0, self.table.shape[1]):\n                print(self.table.item(f, c), end=\" \", file=self.redirectPrintTo)\n            print(\"\", file=self.redirectPrintTo)\n\n    def finishedGame(self) -> bool:\n        for f in range(0, self.table.shape[0]):\n            for c in range(0, self.table.shape[1]):\n                if self.table.item(f, c) == 0:\n                    return False\n        self.winner = -1\n        return True\n\n    def findWinnerAtDiag(self, table: np.matrix) -> int:\n        for f in range(0, table.shape[0]):\n          #  -table.shape[0]+1 is the last principal diagonal of field\n          #  table.shape[1]-1 is the last principal diagonal of cols\n          # self.lengthOfLine \n          for numDiag in range(-table.shape[0]+1, table.shape[1]-1):\n            diag = np.diag(table, k=numDiag)\n            if self.lengthOfLine <= diag.shape[0]:\n                player = self.findWinnerAtfields(np.matrix(diag))\n                if player != 0:\n                    return player\n        return 0\n  \n    def findWinnerAtfields(self, table: np.matrix, redirect=sys.stdout) -> int:\n        countOfLine=0\n        player=0\n        weHaveAWinner=False\n\n        # self.formatTable(table)\n        for f in range(0, table.shape[0]):\n            player=0\n            countOfLine=0\n\n            for c in range(0, table.shape[1]):\n                cell = table.item(f, c)\n                # print(cell, end=\" \")\n                if cell == 0:\n                    countOfLine=0\n                    player=0\n                else:\n                    if player == cell:\n                        countOfLine+=1\n                    else:\n                        player=cell\n                        countOfLine=1\n                    if countOfLine == self.lengthOfLine:\n                        weHaveAWinner=True\n                        break\n            # print(\"\")\n            if weHaveAWinner:\n                break\n        if weHaveAWinner:\n            return player\n        return 0\n\n    def checkWinnerAt(self, findWinnerAt, table, foundMessage):\n        winnerAt = findWinnerAt(table) # find winner at function passed\n        if winnerAt == 0:\n            return False\n        else:\n            self.winner = winnerAt\n            print(foundMessage+ \" \" + str(winnerAt), file=self.redirectPrintTo)\n            return True\n    \n    def checkIfWinner(self) -> bool:\n        print(\"...checking...\", file=self.redirectPrintTo)\n        \n        # check cols and fields\n        return self.checkWinnerAt(self.findWinnerAtfields, self.table, \"we have a fields winner!!!,\") \\\n            or self.checkWinnerAt(self.findWinnerAtfields, self.table.transpose(), \"we have a cols winner!!!,\") \\\n            or self.checkWinnerAt(self.findWinnerAtDiag, self.table, \"we have a principal diagonal winner!!!,\") \\\n            or self.checkWinnerAt(self.findWinnerAtDiag, self.table.dot(self.secondTransform), \"we have a secondary diagonal winner!!!,\")\n    \n    def scalar_to_action(self, scalar):\n        maxField = self.fields\n        maxCols = self.cols\n        \n        field = math.floor(scalar/maxField)\n        cols = scalar % maxCols\n        \n        return (field, cols)\n    \n    # def step(self, action, idPlayer: int, players):\n    def step(self, action):\n        # check if you have new points to use\n        self.finishedGame()\n        self.checkIfWinner()\n\n        if self.winner == 0:\n            if type(action) == tuple:\n                self.crossOutCell(action[0], action[1], self.turn)\n            elif type(action) == int:\n                new_action = self.scalar_to_action(action)\n                self.crossOutCell(new_action[0], new_action[1], self.turn)\n            else:\n                print(\"error\")\n\n            self.finishedGame()\n            self.checkIfWinner()\n            \n        return self.getReward(self.turn)\n    \n    def getReward(self, idPlayer):\n        reward = 0\n        done = False\n        info  = \"Playing...\"\n                  \n        if self.winner != 0:\n            if self.winner == -1:\n                reward = 0\n                done=True\n                info = {\"We dont have a winner, draw :(\", self.winner}\n            elif self.winner == idPlayer:\n                reward = 1\n                done=True\n                info = {\"You win!\", self.winner}\n            elif self.winner != idPlayer:\n                reward = -1\n                done=True\n                info = {\"You lose\", self.winner}\n            else:\n                print(\"unknown winner option\")\n\n            \n        return self.table, reward, done, info\n    \n    def render(self, players=None, mode=\"human\"):\n        if mode == \"human\":\n            self.formatTable(players, showHeader=True)\n            \n#register(\n#    id='gym_examples/tictactoe',\n#    entry_point='gym_examples.envs:tictactoe',\n#    max_episode_steps=300,\n#)","metadata":{"_uuid":"d381eb44-179e-4466-a7e5-679c2888736a","_cell_guid":"2ea83875-54f2-4954-bb73-1c560de049ef","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2022-06-29T02:25:39.382199Z","iopub.execute_input":"2022-06-29T02:25:39.382653Z","iopub.status.idle":"2022-06-29T02:25:39.436155Z","shell.execute_reply.started":"2022-06-29T02:25:39.382617Z","shell.execute_reply":"2022-06-29T02:25:39.435106Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next i add all logic. First at all Player class:","metadata":{}},{"cell_type":"code","source":"class Player:\n    def __init__(self, idPlayer: int, name: str, tictactoe: TicTacToe):\n        if name == \"\":\n              self.name = input(\"Player insert your name:\")\n        self.name = name\n        self.tictactoe = tictactoe\n        self.idPlayer = idPlayer\n        self.n_state = None\n        self.reward = None\n        self.done = False\n        self.info = None\n        \n    def play(self, players, action=None):\n        field = 0\n        column = 0\n        if not self.done:\n            if action == None:\n                field = int(input('Enter field:'))\n                column = int(input('Enter column:'))\n            else:\n                field = action[0]\n                cols = action[1]\n        n_state, reward, done, info = self.tictactoe.step((field, column))\n\n        self.save(n_state, reward, done, info)\n    \n    def save(self, n_state, reward, done, info):\n        self.n_state = n_state\n        self.reward = reward\n        self.done = done\n        self.info = info\n        \n    def showResults(self):\n        return self.n_state, self.reward, self.done, self.info\n    \n    def showDescription(self):\n        print('id:{} State:{} Reward:{} Done:{} Info:{}'.format(self.idPlayer, self.n_state, self.reward, self.done, self.info), file=self.tictactoe.redirectPrintTo)\n\n    def getPlayer(self):\n        return self","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:25:40.946196Z","iopub.execute_input":"2022-06-29T02:25:40.946623Z","iopub.status.idle":"2022-06-29T02:25:40.966582Z","shell.execute_reply.started":"2022-06-29T02:25:40.946592Z","shell.execute_reply":"2022-06-29T02:25:40.965225Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"markdown","source":"now we add random AI","metadata":{}},{"cell_type":"code","source":"# deprecated: AI_Reinforcement_Random remplace it\nclass AIRandom(Player):\n    def __init__(self, idPlayer: int, name: str, tictactoe: TicTacToe):\n        super().__init__(idPlayer, name, tictactoe)\n \n    def play(self, players):\n        action = (np.random.randint(0, self.tictactoe.fields), np.random.randint(0, self.tictactoe.cols))\n        n_state, reward, done, info = self.tictactoe.step(action)\n\n        self.save(n_state, reward, done, info)\n     ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:25:41.202256Z","iopub.execute_input":"2022-06-29T02:25:41.202983Z","iopub.status.idle":"2022-06-29T02:25:41.212760Z","shell.execute_reply.started":"2022-06-29T02:25:41.202925Z","shell.execute_reply":"2022-06-29T02:25:41.211696Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"Ai reinforcement with openai gym package config","metadata":{}},{"cell_type":"code","source":"class AI_Reinforcement_Random(Player):\n    def __init__(self, idPlayer: int, name: str, tictactoe: TicTacToe):\n        super().__init__(idPlayer, name, tictactoe)\n        self.score = 0\n        # self.episode=1\n\n    def play(self, players):\n        # tictactoe.render(players)\n\n        action = self.tictactoe.action_space.sample()\n\n        n_state, reward, done, info = self.tictactoe.step(action)\n        \n        self.save(n_state, reward, done, info)\n\n    def showDescription(self):\n        print('id:{} Episode:{} Reward:{} Info:{}'.format(self.idPlayer, 1, self.reward, self.info), file=self.tictactoe.redirectPrintTo)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:25:42.234217Z","iopub.execute_input":"2022-06-29T02:25:42.235454Z","iopub.status.idle":"2022-06-29T02:25:42.245288Z","shell.execute_reply.started":"2022-06-29T02:25:42.235389Z","shell.execute_reply":"2022-06-29T02:25:42.244158Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"I realized that use tictactoc env dont allow to play inside step. Hence i create judge class between tictactoe class","metadata":{}},{"cell_type":"code","source":"class Judge:\n    def __init__(self, tictactoe, players):\n        self.tictactoe = tictactoe\n        self.players = players\n        # self.pointUpdated = np.zeros(2) # 0: field, 1 col\n        # self.player = 0\n        \n    def invoke(self):\n        done = False\n        counter = 0 \n        while not done and counter < 100:\n            \n            # - 1 because the user of turn 0 doesnt exist\n            player = self.players[self.tictactoe.turn - 1]\n            \n            # render game \n            self.tictactoe.render(self.players)\n \n            player.play(self.players)\n            n_state, reward, done, info = self.tictactoe.getReward(player.idPlayer)\n            \n            counter+=1\n            \n        # tells all users who wins\n        for player in self.players:\n            n_state, reward, done, info = self.tictactoe.getReward(player.idPlayer)\n            player.save(n_state, reward, done, info)\n            \n            player.showDescription()\n            ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:25:42.496648Z","iopub.execute_input":"2022-06-29T02:25:42.497158Z","iopub.status.idle":"2022-06-29T02:25:42.507018Z","shell.execute_reply.started":"2022-06-29T02:25:42.497097Z","shell.execute_reply":"2022-06-29T02:25:42.505931Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"i create a automated playing function","metadata":{}},{"cell_type":"code","source":"def playTwoPlayers(player1Class, player2Class, nameP1, nameP2, allowDebug=False):\n  \n    tictactoe = TicTacToe(2, np.zeros((3, 3), np.int32), 3, allowDebug)\n    #tictactoe = TicTacToe(2, np.array([\n    #    [2, 1, 2],\n    #    [1, 1, 0],\n    #    [2, 2, 1]\n    #], np.int32), 3, allowDebug)\n    \n    # start at one because 0 represent an empty cell and should be positive integers \n    player1 = player1Class(1, nameP1, tictactoe) \n    player2 = player2Class(2, nameP2, tictactoe)\n\n    judge = Judge(tictactoe, [player1, player2])\n    \n    # tictactoe.reset()\n    \n    judge.invoke()\n\n    return [ tictactoe.history, tictactoe.winner]\n    ","metadata":{"_uuid":"b71794f4-cfe6-4262-9cc4-9fcb8ff93c71","_cell_guid":"40214fd7-1dcd-4ce4-bd5c-7beb71d6bf50","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-29T02:25:42.769880Z","iopub.execute_input":"2022-06-29T02:25:42.770744Z","iopub.status.idle":"2022-06-29T02:25:42.780683Z","shell.execute_reply.started":"2022-06-29T02:25:42.770692Z","shell.execute_reply":"2022-06-29T02:25:42.779295Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"markdown","source":"Now we put them to compete and see the odds of winning, losing and drawing. Using the following function","metadata":{}},{"cell_type":"code","source":"def createPlot(historyAndWinner, player1Class, player2Class, winnerAtitle, winnerBtitle, redirect=open(os.devnull, 'w'), num_of_games=300):\n    batch = 10\n    number_that_divides_list = num_of_games / batch\n\n    winnerAList = np.zeros(shape=batch, dtype=float)\n    drawList = np.zeros(shape=batch, dtype=float)\n    winnerBList = np.zeros(shape=batch, dtype=float)\n\n    num_played = 0\n\n    winnerA= 0\n    draw = 0\n    winnerB= 0\n\n    for i in range(0, num_of_games):\n        history, winner = historyAndWinner(player1Class, player2Class, winnerAtitle, winnerBtitle)\n        \n        # add probabilities points to winnerA, draw and winnerB \n        if winner == 1:\n            winnerA = winnerA + 1\n        elif winner == 2:\n            winnerB = winnerB + 1\n        else:\n            draw = draw + 1\n\n        # save proxination\n        if num_played % number_that_divides_list == 0 and num_played != 0:\n            pos = int(num_played/number_that_divides_list)\n\n            winnerAList.itemset(pos, winnerA/num_played)\n            drawList.itemset(pos, draw/num_played)\n            winnerBList.itemset(pos, winnerB/num_played)\n\n            print(str(num_played) + \"/\" + str(num_of_games) + \" games\", file=redirect)\n        num_played = num_played + 1\n    \n    \n    x = np.linspace(batch, num_of_games, batch)\n    y= np.ones(batch)\n    \n    plt.figure(figsize=(13, 9))\n    plt.title(\"Probabilities for \" + winnerAtitle + \" vs \" + winnerBtitle)\n    \n    plt.xlabel(\"games\") \n    plt.ylabel(\"probability\")\n    \n    plt.plot(x, winnerAList, 'r', label=winnerAtitle)\n    plt.plot(x, drawList, 'g', label=\"draw\")\n    plt.plot(x, winnerBList, 'b', label=winnerBtitle)\n    \n    plt.plot(x, winnerAList[batch-1]*y, 'r', label='limit ' + winnerAtitle + ': ' + str(winnerAList[batch-1]))\n    plt.plot(x, drawList[batch-1]*y, 'g', label='limit draw ' + str(drawList[batch-1]))\n    plt.plot(x, winnerBList[batch-1]*y, 'b', label='limit ' + winnerBtitle + ': ' + str(winnerBList[batch-1]))\n    \n    plt.legend(loc='upper left')\n    \n    plt.show()\n\n# createPlot(historyAndWinner=playTwoPlayers, title=\"AI Random vs Random\")\n# createPlot(historyAndWinner=playAIRandomVSReinforcement, title=\"AI Random vs AI_Reinforcement_Random\")\n# createPlot(historyAndWinner=playReinforcementVSReinforcement, title=\"AI_Reinforcement_Random vs AI_Reinforcement_Random\")\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-29T02:25:43.818432Z","iopub.execute_input":"2022-06-29T02:25:43.818889Z","iopub.status.idle":"2022-06-29T02:25:43.839269Z","shell.execute_reply.started":"2022-06-29T02:25:43.818853Z","shell.execute_reply":"2022-06-29T02:25:43.838290Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"createPlot(\n    playTwoPlayers,\n    AIRandom,\n    AIRandom,\n    \"AIRandom\",\n    \"AIRandom1\",\n    redirect=sys.stdout,\n    num_of_games=130\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T01:24:26.109577Z","iopub.execute_input":"2022-06-29T01:24:26.110090Z","iopub.status.idle":"2022-06-29T01:24:33.239331Z","shell.execute_reply.started":"2022-06-29T01:24:26.110051Z","shell.execute_reply":"2022-06-29T01:24:33.238155Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"createPlot(\n    playTwoPlayers,\n    AIRandom,\n    AI_Reinforcement_Random,\n    \"AI Random\",\n    \"AI_Reinforcement_Random\",\n    redirect=sys.stdout,\n    num_of_games=200\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T01:24:33.240945Z","iopub.execute_input":"2022-06-29T01:24:33.242075Z","iopub.status.idle":"2022-06-29T01:24:43.240617Z","shell.execute_reply.started":"2022-06-29T01:24:33.242018Z","shell.execute_reply":"2022-06-29T01:24:43.239307Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"createPlot(\n    playTwoPlayers,\n    AI_Reinforcement_Random,\n    AI_Reinforcement_Random,\n    \"AI_Reinforcement_Random\",\n    \"AI_Reinforcement_Random\",\n    redirect=sys.stdout,\n    num_of_games=130\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T01:24:43.242380Z","iopub.execute_input":"2022-06-29T01:24:43.243120Z","iopub.status.idle":"2022-06-29T01:24:50.226349Z","shell.execute_reply.started":"2022-06-29T01:24:43.243068Z","shell.execute_reply":"2022-06-29T01:24:50.224635Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"look how bad he plays and play against him","metadata":{}},{"cell_type":"code","source":"history, winner = playTwoPlayers(\n    AI_Reinforcement_Random,\n    AI_Reinforcement_Random,\n    \"AI_Reinforcement_Random1\",\n    \"AI_Reinforcement_Random2\",\n    allowDebug=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:25:51.154601Z","iopub.execute_input":"2022-06-29T02:25:51.155224Z","iopub.status.idle":"2022-06-29T02:25:51.213225Z","shell.execute_reply.started":"2022-06-29T02:25:51.155100Z","shell.execute_reply":"2022-06-29T02:25:51.212277Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"try:\n    playTwoPlayers(\n        Player,\n        AI_Reinforcement_Random,\n        \"Me\",\n        \"Reinforcement\",\n        allowDebug=True\n\n    )\nexcept:\n    print(\"input error\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T01:24:50.314498Z","iopub.execute_input":"2022-06-29T01:24:50.314881Z","iopub.status.idle":"2022-06-29T02:01:05.893857Z","shell.execute_reply.started":"2022-06-29T01:24:50.314848Z","shell.execute_reply":"2022-06-29T02:01:05.892502Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# 2. I create a class that makes its own decisions with a neural network","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras import losses, metrics, optimizers\n\nfrom keras.models import Sequential\nfrom keras.layers import MaxPooling2D, \\\n    Dropout, \\\n    Dense, \\\n    Flatten, \\\n    Convolution2D as Conv2D, \\\n    Reshape\nfrom keras.utils.vis_utils import plot_model\n\nfrom tensorflow.keras.optimizers import SGD\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom pathlib import Path\nfrom IPython.display import Image","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:26:02.010690Z","iopub.execute_input":"2022-06-29T02:26:02.011185Z","iopub.status.idle":"2022-06-29T02:26:02.019153Z","shell.execute_reply.started":"2022-06-29T02:26:02.011147Z","shell.execute_reply":"2022-06-29T02:26:02.018042Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":"Note: AIBestDecision in Version 17 has been trained with a random player, therefore winner prediction are too random (the win prediction of both are 0, verified in practice). Hence try to predict the winner is a bad strategy. Now,\ntry instead of making a prediction, I predict the best option straight","metadata":{}},{"cell_type":"code","source":"class AIBestDecision(AI_Reinforcement_Random):\n    _aiBestDecision = None\n    \n    def __init__(self, idPlayer: int, name: str, tictactoe: TicTacToe, \\\n            datasetFile = '/kaggle/input/tictactoe-win-movement/tictactoe_win_movement.csv', \\\n            datasetFileUpdates = '/kaggle/working/tictactoe_win_movement.csv', \\\n            build=True\n                ):\n        super().__init__(idPlayer, name, tictactoe)\n        \n\n        self.inputTable = []\n        self.prob_choose = []\n        self.turn_choose  = []\n        self.next_table = []\n        self.network = None\n        self.initialized = False\n\n        self.metrics = None\n        self.datasetFile = datasetFile\n        self.datasetFileUpdates = datasetFileUpdates\n\n        self.numOfGames = 0\n        self.initial_structure(build)\n        \n    def Singleton(idPlayer: int = -1, name: str = \"example\", tictactoe: TicTacToe = None, \\\n            datasetFile = '/kaggle/input/tictactoe-win-movement/tictactoe_win_movement.csv', \\\n            datasetFileUpdates = '/kaggle/working/tictactoe_win_movement.csv', \\\n            build=True\n                ):\n        if AIBestDecision._aiBestDecision == None:\n            AIBestDecision._aiBestDecision = AIBestDecision(\n                idPlayer,\n                name,\n                tictactoe,\n                datasetFile,\n                datasetFileUpdates,\n                build\n            )\n        agent = AIBestDecision(\n            idPlayer,\n            name,\n            tictactoe,\n            datasetFile,\n            datasetFileUpdates,\n            False\n        )\n        agent.network = AIBestDecision._aiBestDecision.network\n        \n        return agent\n        \n    def initial_structure(self, build):\n        \n        if not self.initialized and build:\n            \n            # create dataset file\n            self.createDataset()\n\n            # create data\n            self.collect()\n                        \n            # save already created data\n            self.saveToDataset()\n            \n            # get already saved data\n            self.getDataset()\n            \n            # create model, compile and fit\n            self.model()\n            self.compileNetwork()\n            self.fit()\n            \n            self.initialized = True\n        \n    def swap_id_player_in_table(self, arr, idplayerWinner, idPlayerLooser):\n        arrAux = np.matrix(arr)\n        \n        np.place(arrAux, arrAux == idplayerWinner, -idplayerWinner)\n        np.place(arrAux, arrAux == idPlayerLooser, idplayerWinner)\n        np.place(arrAux, arrAux == -idplayerWinner, idPlayerLooser)\n    \n        return arrAux\n    \n    def collect(self):\n        for idGame in range(0, self.numOfGames):\n            # history, rewardsPerUser = self.randomPlay()\n            history, winner = self.randomPlay()\n            length_of_history = len(history.numpy())\n\n            for i in range(0, length_of_history - 1):\n\n                tableBefore = np.matrix(history[i])\n                tableAfter = np.matrix(history[i+1])\n\n                # if tableAfter is not zero, T != 0\n                if not np.equal(tableAfter, np.zeros((3,3))).all():\n\n                    tableChange = (tableAfter - tableBefore)\n\n                    # 1/self.idPlayer normalze the matrix ( all 0 or 1)\n                    turn = tableChange.sum()\n                    tableChangeNormalized = (1/turn) * tableChange\n\n                    # If the winner plays and he is not me, do roles reverse.\n                    # that is instead of ignoring \n                    if winner != 1:\n                        \n                        # learn from the rival decisions\n                        tableBefore = self.swap_id_player_in_table(tableBefore, winner, 1)\n                        tableAfter= self.swap_id_player_in_table(tableAfter, winner, 1)\n                        \n                    # if win, draw or lose in the actual table, too in identity table\n                    self.inputTable.append(tableBefore)\n                    self.prob_choose.append(tableChangeNormalized)\n                    self.turn_choose.append(turn)\n                    self.next_table.append(tableAfter)\n    \n                    # if win, draw or lose in the actual table, too in table transpose\n                    self.inputTable.append(tableBefore.copy().transpose())\n                    self.prob_choose.append(tableChangeNormalized.copy().transpose())\n                    self.turn_choose.append(turn)\n                    self.next_table.append(tableAfter.copy().transpose())\n                    \n                    # if win, draw or lose in the actual table, too in table flip \n                    self.inputTable.append(tableBefore.copy().dot(self.tictactoe.secondTransform))\n                    self.prob_choose.append(tableChangeNormalized.copy().dot(self.tictactoe.secondTransform))\n                    self.turn_choose.append(turn)\n                    self.next_table.append(tableAfter.copy().dot(self.tictactoe.secondTransform))\n                    \n                    \n                    # if win, draw or lose in the actual table, too in transpose and flips\n                    \n                    self.inputTable.append(\n                        tableBefore.copy()\n                            .dot(self.tictactoe.secondTransform)\n                            .transpose()\n                    )\n                    self.prob_choose.append(\n                        tableChangeNormalized.copy()\n                            .dot(self.tictactoe.secondTransform) \n                            .transpose()\n                    )\n                    self.turn_choose.append(turn)\n                    self.next_table.append(\n                        tableAfter.copy()\n                            .dot(self.tictactoe.secondTransform)\n                            .transpose()\n                    )\n                    \n                    \n    def createDataset(self):\n        # /kaggle/working\n        # if not Path(self.datasetFileUpdates).is_file():\n        csvBestDecision = DataFrame(data=None, index=None, columns=[\"table\", \"table_prob_choose\", \"turn_choose\", \"next_table\"] ,dtype=None, copy=None)\n        csvBestDecision.to_csv(self.datasetFileUpdates, index=None) # save to notebook output\n            \n    def getDataset(self):\n        csvBestDecision = pd.read_csv(self.datasetFile) # load from notebook input\n        \n        self.inputTable = csvBestDecision[\"table\"].to_list()\n        self.prob_choose =  csvBestDecision[\"table_prob_choose\"].to_list()\n        self.turn_choose =  csvBestDecision[\"turn_choose\"].to_list()\n        \n        for i in range(0, len(self.inputTable)):\n            # tableAux = np.matrix(AIBestDecision.inputTable[i], dtype= np.int32)\n            # prob_choose_aux = np.matrix(AIBestDecision.prob_choose[i], dtype=np.int32)\n\n            self.inputTable[i] = np.reshape(\n                np.matrix(self.inputTable[i], dtype= np.int32),\n                (3, 3)\n            )\n            self.prob_choose[i] = np.reshape(\n                np.matrix(self.prob_choose[i], dtype=np.int32),\n                (1, 9)\n            )\n            \n        self.inputTable = np.array(self.inputTable)\n        self.prob_choose =  np.array(self.prob_choose)\n        self.turn_choose =  np.array(self.turn_choose)\n\n        # AIBestDecision.inputTable = tf.convert_to_tensor(AIBestDecision.inputTable)\n        # AIBestDecision.prob_choose =  tf.convert_to_tensor(AIBestDecision.prob_choose)\n        \n        print(\"Data size: \" + str(len(csvBestDecision[\"table_prob_choose\"])))\n        \n    def saveToDataset(self):\n        data = {\n            'table': self.inputTable,\n            'table_prob_choose': self.prob_choose,\n            \"turn_choose\": self.turn_choose,\n            \"next_table\": self.next_table\n        }\n                \n        # Make data frame of above data\n        df = pd.DataFrame(data)\n \n        # append data frame to CSV file\n        df.to_csv(self.datasetFileUpdates, mode='a', header=False, index=False)\n        \n    def play(self, players):\n        table = self.tictactoe.table.copy()\n        action = (0, 0)\n        probToWin = 0\n        \n        # adapt the table to the train data\n        if self.idPlayer != 1:\n            table = self.swap_id_player_in_table(table, self.idPlayer, 1)\n            \n        predict = self.network.predict(np.array(table).reshape(\n            (1, self.tictactoe.fields, self.tictactoe.cols)\n        )).reshape((self.tictactoe.fields, self.tictactoe.cols))\n        \n        for f in range(0, self.tictactoe.fields):\n            for c in range(0, self.tictactoe.cols):\n                \n                # search empty spaces in the table \n                if table.item(f,c) == 0:\n                    \n                    # search best option\n                    if probToWin < predict.item(f, c):\n                        probToWin = predict.item(f, c)\n                        action = (f, c)\n                        \n        n_state, reward, done, info = self.tictactoe.step(action)\n        \n        self.save(n_state, reward, done, info)\n        \n    def randomPlay(self):\n        # Player\n        tictactoe = TicTacToe(2, np.zeros((3, 3), np.int32), 3, allowDebug=False)  # type: ignore\n\n        player1 = AI_Reinforcement_Random(1, \"Train1\", tictactoe)\n        player2 = AI_Reinforcement_Random(2, \"Train2\", tictactoe)\n\n        judge = Judge(tictactoe, [player1, player2])\n\n        tictactoe.reset()\n        judge.invoke()\n\n        # return [ tictactoe.history, [ player1.reward, player2.reward ] ]\n        return tictactoe.history, tictactoe.winner\n    \n    def divide_data(self):\n        trainBatch = int(2*len(self.prob_choose)/3)\n        print(trainBatch)\n        \n        input_train = self.inputTable[0:trainBatch]\n        prob_choose_train = self.prob_choose[0:trainBatch]\n     \n        input_val = self.inputTable[trainBatch:len(self.prob_choose)]\n        prob_choose_val = self.prob_choose[trainBatch:len(self.prob_choose)]\n            \n        print(\"train\")\n        print(len(input_train))\n        print(len(prob_choose_train))\n        \n        print(\"eval\")\n        print(len(input_val))\n        print(len(prob_choose_val))\n\n        return trainBatch, input_train, prob_choose_train, input_val, prob_choose_val\n    \n    def model(self):\n        \n        self.network = models.Sequential()\n        self.network.add(Dense(3, input_shape=(3, 3), activation='relu'))\n        \n        self.network.add(Dense(6, activation='relu')) # ...\n        self.network.add(Dense(9, activation='relu')) # 30 \n        self.network.add(Dense(30, activation='relu')) # 30  \n        self.network.add(Dense(30, activation='relu')) # 30  \n        self.network.add(Dense(30, activation='relu'))\n        self.network.add(Dense(18, activation='relu'))\n        \n        \n        # AIBestDecision.network.add(Dense(9, activation='sigmoid'))\n        self.network.add(Reshape((1, 18*3)))\n        self.network.add(Dense(9, activation='sigmoid'))\n        \n        self.network.summary()\n        \n    def compileNetwork(self):\n\n        self.network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    def fit(self):\n        trainBatch, input_train, prob_choose_train, input_val, prob_choose_val = self.divide_data()\n        num_epoch = 35\n\n        self.metrics = self.network.fit(\n            np.array(input_train),\n            np.array(prob_choose_train),\n            epochs=num_epoch,\n            batch_size=int(trainBatch/num_epoch), # /num_epoch\n             validation_data=(\n                np.array(input_val),\n                np.array(prob_choose_val)\n            ),\n            verbose=1\n        )\n        \n        self.seeMetrics()\n        \n    def seeMetrics(self):\n        \n        # Plot the accuracy curves\n        plt.plot(self.metrics.history['accuracy'],'bo')\n        plt.plot(self.metrics.history['val_accuracy'],'rX')\n        plt.title('model accuracy')\n        plt.ylabel('accuracy')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.grid()\n        plt.show()\n        \n        # summarize history for loss\n        plt.plot(self.metrics.history['loss'],'bo')\n        plt.plot(self.metrics.history['val_loss'],'rX')\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.grid()\n        plt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-29T02:26:03.472006Z","iopub.execute_input":"2022-06-29T02:26:03.472990Z","iopub.status.idle":"2022-06-29T02:26:03.539616Z","shell.execute_reply.started":"2022-06-29T02:26:03.472927Z","shell.execute_reply":"2022-06-29T02:26:03.538216Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history, winner = playTwoPlayers(\n    AIBestDecision.Singleton,\n    AI_Reinforcement_Random,\n    \"Best decision\",\n    \"Reforce\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:26:04.991755Z","iopub.execute_input":"2022-06-29T02:26:04.992984Z","iopub.status.idle":"2022-06-29T02:28:11.445413Z","shell.execute_reply.started":"2022-06-29T02:26:04.992930Z","shell.execute_reply":"2022-06-29T02:28:11.444365Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"# plot_model(AIBestDecision.network, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n# Image(\"model_plot.png\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:03:02.978079Z","iopub.execute_input":"2022-06-29T02:03:02.978557Z","iopub.status.idle":"2022-06-29T02:03:02.984200Z","shell.execute_reply.started":"2022-06-29T02:03:02.978519Z","shell.execute_reply":"2022-06-29T02:03:02.982344Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# table = np.matrix([0], dtype=np.int32)\nresult = np.matrix([[1,0,0],[1,0,0],[2,2,0]])\nresult = np.matrix([[0,0,0],[0,0,0],[0,0,0]])\n\npredict = AIBestDecision.Singleton().network.predict(np.array(result).reshape((1, 3, 3))).reshape((3, 3))\nprint(predict)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:03:02.986066Z","iopub.execute_input":"2022-06-29T02:03:02.986535Z","iopub.status.idle":"2022-06-29T02:03:03.002880Z","shell.execute_reply.started":"2022-06-29T02:03:02.986499Z","shell.execute_reply":"2022-06-29T02:03:03.001618Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"now, with the logic done we can make a network of keras","metadata":{}},{"cell_type":"code","source":"history, winner = playTwoPlayers(\n    AIBestDecision.Singleton,\n    AI_Reinforcement_Random,\n    \"Best decision\",\n    \"Reforce\"\n)\nprint(history)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:11.447311Z","iopub.execute_input":"2022-06-29T02:28:11.447708Z","iopub.status.idle":"2022-06-29T02:28:11.520678Z","shell.execute_reply.started":"2022-06-29T02:28:11.447665Z","shell.execute_reply":"2022-06-29T02:28:11.518676Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"# 3. Now we look at the performance of best decision model","metadata":{}},{"cell_type":"code","source":"createPlot(\n    playTwoPlayers,\n    AIBestDecision.Singleton,\n    AI_Reinforcement_Random,\n    \"Best decision\",\n    \"AI_Reinforcement_Random\",\n    redirect=sys.stdout,\n    num_of_games=200\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:03:03.080735Z","iopub.execute_input":"2022-06-29T02:03:03.081249Z","iopub.status.idle":"2022-06-29T02:03:15.405477Z","shell.execute_reply.started":"2022-06-29T02:03:03.081200Z","shell.execute_reply":"2022-06-29T02:03:15.404216Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"51% Best decision - 5% draw - 37% AI_reinforcement\n","metadata":{}},{"cell_type":"code","source":"history, winner = playTwoPlayers(\n    AIBestDecision.Singleton,\n    AIBestDecision.Singleton,\n    \"Best decision\",\n    \"Best decision1\"\n)\nprint(history)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:11.522300Z","iopub.execute_input":"2022-06-29T02:28:11.523105Z","iopub.status.idle":"2022-06-29T02:28:11.620957Z","shell.execute_reply.started":"2022-06-29T02:28:11.523043Z","shell.execute_reply":"2022-06-29T02:28:11.620164Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"createPlot(\n    playTwoPlayers,\n    AIBestDecision.Singleton,\n    AIBestDecision.Singleton,\n    \"Best decision\",\n    \"Best decision\",\n    redirect=sys.stdout,\n    num_of_games=120\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:11.623866Z","iopub.execute_input":"2022-06-29T02:28:11.624403Z","iopub.status.idle":"2022-06-29T02:28:21.705592Z","shell.execute_reply.started":"2022-06-29T02:28:11.624353Z","shell.execute_reply":"2022-06-29T02:28:21.704545Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"markdown","source":"best decision model cant beat himself, what should get it is a plot with all draws. This in addition of the previus plot, tells us that best decision model prevent draws. Because the model is smart, would tend to tie","metadata":{}},{"cell_type":"code","source":"try:\n    history, winner = playTwoPlayers(\n        Player,\n        AIBestDecision.Singleton,\n        \"Me\",\n        \"Best decision\",\n        allowDebug=True\n    )\nexcept:\n    print(\"input error\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:03:23.201118Z","iopub.execute_input":"2022-06-29T02:03:23.201703Z","iopub.status.idle":"2022-06-29T02:03:25.248749Z","shell.execute_reply.started":"2022-06-29T02:03:23.201666Z","shell.execute_reply":"2022-06-29T02:03:25.247393Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"Now, i will build an agent","metadata":{}},{"cell_type":"code","source":"from rl.agents import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n\n# import tensorflow as tf\n\nfrom tensorflow import keras\nfrom keras import layers\n\nfrom keras import losses, metrics, optimizers\n\nfrom tensorflow.keras.optimizers import Adam \nfrom keras.models import Sequential\nfrom keras.layers import MaxPooling2D, \\\n    Dropout, \\\n    Dense, \\\n    Flatten, \\\n    Convolution2D as Conv2D, \\\n    Reshape\nfrom keras.utils.vis_utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:21.707193Z","iopub.execute_input":"2022-06-29T02:28:21.707654Z","iopub.status.idle":"2022-06-29T02:28:21.714436Z","shell.execute_reply.started":"2022-06-29T02:28:21.707611Z","shell.execute_reply":"2022-06-29T02:28:21.713599Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"# class AgentQLearning(AIBestDecision):\nclass AgentQLearning(AI_Reinforcement_Random):\n    _agent_q_learning = None\n    \n    def __init__(self, idPlayer, name, tictactoe, build=True):\n        super().__init__(idPlayer, name, tictactoe)\n        self.states = self.tictactoe.fields * self.tictactoe.cols\n        self.actions = (self.tictactoe.action_space).n\n        #self.actions =  [\n        #    self.tictactoe.action_space[0].n,\n        #    self.tictactoe.action_space[1].n\n        #]\n        \n        self.policy = None\n        self.memory = None\n        self.dqn = None\n        self.model = None\n        \n        # if self.policy == None:\n        # self.build_agent()\n        if build:\n            self.build_model()\n            self.train()\n            self.test()\n    \n    def Singleton(idPlayer: int = -1, name: str = \"example\", tictactoe= None):\n        if AgentQLearning._agent_q_learning == None:\n            AgentQLearning._agent_q_learning = AgentQLearning(idPlayer, name, tictactoe)\n        \n        agent = AgentQLearning(\n            idPlayer,\n            name,\n            tictactoe,\n            build = False\n        )\n        agent.model = AIBestDecision._aiBestDecision.model\n        \n        return agent\n    \n    def build_model(self):\n        self.model = Sequential()\n        \n        # self.model.add(Dense(self.states, input_shape=(1, self.states), activation='relu'))\n        # self.model.add(Flatten(input_shape=(1, 2)))\n        \n        # self.model.add(Reshape((1, 2)))\n        #self.model.add(Dense(2, input_shape=(1, 2), activation='relu'))\n        self.model.add(Flatten(input_shape=(1, 9)))\n\n        self.model.add(Dense(24, activation='relu'))\n        self.model.add(Dense(24, activation='relu'))\n        self.model.add(Dense(24, activation='relu'))\n\n        self.model.add(Flatten(input_shape=(1, 24)))\n        # self.model.add(Reshape((1, 142)))\n        self.model.add(Dense(9, activation='sigmoid'))\n        \n        self.model.summary()\n    \n    def train(self):\n        self.dqn = self.build_agent(\n            self.model,\n            self.states\n        )\n        \n\n        self.dqn.compile(\n            Adam(\n                learning_rate=1e-3\n            ),\n            metrics=['mae']\n        )\n        self.dqn.fit(\n            self.tictactoe,\n            nb_steps=10,\n            visualize=True,\n            verbose=1\n            #input_shape=(3,3)\n        )\n        self.model.summary()\n        \n    def test(self):\n        scores = self.dqn.test(self.tictactoe, nb_episodes=100, visualize=True)\n        print(np.mean(scores.history['episode_reward']))\n    \n    def build_agent(self, model, actions):\n        self.policy = BoltzmannQPolicy()\n        self.memory = SequentialMemory(\n            limit=50000,\n            window_length=1\n        )\n        \n        self.dqn = DQNAgent(\n            model=model,\n            memory=self.memory,\n            policy=self.policy,\n            nb_actions=self.actions,\n            nb_steps_warmup=10,\n            target_model_update=1e-2\n        )\n        \n        return self.dqn\n    \n    def play(self, players):\n        table = self.tictactoe.table.copy()\n        action = (0, 0)\n        probToWin = 0\n        \n        # adapt the table to the train data\n        if self.idPlayer != 1:\n            table = self.swap_id_player_in_table(table, self.idPlayer, 1)\n            \n        predict = self.model.predict(np.array(table).reshape(\n            (self.tictactoe.fields, self.tictactoe.cols)\n        )).reshape((self.tictactoe.fields, self.tictactoe.cols))\n        \n        for f in range(0, self.tictactoe.fields):\n            for c in range(0, self.tictactoe.cols):\n                \n                # search empty spaces in the table \n                if table.item(f,c) == 0:\n                    \n                    # search best option\n                    if probToWin < predict.item(f, c):\n                        probToWin = predict.item(f, c)\n                        action = (f, c)\n                        \n        n_state, reward, done, info = self.tictactoe.step(action)\n        \n        self.save(n_state, reward, done, info)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:21.715734Z","iopub.execute_input":"2022-06-29T02:28:21.716825Z","iopub.status.idle":"2022-06-29T02:28:21.747320Z","shell.execute_reply.started":"2022-06-29T02:28:21.716781Z","shell.execute_reply":"2022-06-29T02:28:21.746189Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"print()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:21.748638Z","iopub.execute_input":"2022-06-29T02:28:21.749421Z","iopub.status.idle":"2022-06-29T02:28:21.761379Z","shell.execute_reply.started":"2022-06-29T02:28:21.749384Z","shell.execute_reply":"2022-06-29T02:28:21.760054Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"#try:\nhistory, winner = playTwoPlayers(\n    AgentQLearning.Singleton,\n    AI_Reinforcement_Random,\n    \"AgentQLearning\",\n    \"AI_Reinforcement_Random\"\n)\n#except:\n#    print(\"error\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:28:21.763034Z","iopub.execute_input":"2022-06-29T02:28:21.763481Z","iopub.status.idle":"2022-06-29T02:28:44.642014Z","shell.execute_reply.started":"2022-06-29T02:28:21.763444Z","shell.execute_reply":"2022-06-29T02:28:44.640245Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"createPlot(\n    playTwoPlayers,\n    AgentQLearning.Singleton,\n    AI_Reinforcement_Random,\n    \"Best decision\",\n    \"AI_Reinforcement_Random\",\n    redirect=sys.stdout,\n    num_of_games=200\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T02:03:36.650452Z","iopub.status.idle":"2022-06-29T02:03:36.650882Z","shell.execute_reply.started":"2022-06-29T02:03:36.650695Z","shell.execute_reply":"2022-06-29T02:03:36.650715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}